#!/usr/bin/env python3
"""
Build a POS-tag dictionary
based on the annotated English
Gigaword corpus.
"""
import argparse
import os
import gzip
import pickle
from collections import defaultdict
from lxml import etree
from xml.etree.ElementTree import Element

import logging
logging.basicConfig()
LOG = logging.getLogger()

from nltk.tree import Tree
from intent2.utils.cli_args import existsdir

def nested_dd():
    return defaultdict(int)

def nested_dd_float():
    return defaultdict(float)

class Vocab(dict):

    def add_word(self, word, pos, n=1):
        if word not in self:
            self[word] = {pos: n}
        elif pos not in self[word]:
            self[word][pos] = n
        else:
            self[word][pos] += n

    def calc_probs(self):
        probs = defaultdict(nested_dd_float)
        for word in self:
            total = 0
            for tag in self[word]:
                total += self[word][tag]
            for tag in self[word]:
                probs[word][tag] = self[word][tag]/total
        return probs

    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump(dict(self), f)

    def save_probs(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self.calc_probs(), f)


def parse_file(xml_path: str, vocab: Vocab):
    """
    Given one of the files in the gigaword
    corpus, load it and scan for POS tags.
    """
    LOG.info('Attempting to parse "{}"'.format(xml_path))
    with gzip.GzipFile(xml_path, 'r') as xml_f:
        cur_doc_id = None
        for event, element in etree.iterparse(xml_f): # type: None, Element
            if element.tag == 'DOC':
                cur_doc_id = element.attrib['id']
                LOG.info('Parsing DOC id "{}'.format(cur_doc_id))
            if element.tag in {'parse'}:
                # trees = [t for t in element.text.split('\n') if t.strip()]
                # for tree in trees:
                parse = Tree.fromstring(element.text)
                for word, pos in parse.pos():
                    vocab.add_word(word, pos)




def parse_corpus(corpus_root: str, output_path: str, output_probs: str):
    vocab = Vocab()
    xml_dir = os.path.join(corpus_root, 'data/xml')
    xml_paths = [os.path.join(xml_dir, filename) for filename in os.listdir(xml_dir)]
    for xml_path in xml_paths:

        # Update the vocab file with counts from all the docs in that file.
        parse_file(xml_path, vocab)
        LOG.info('Writing out vocab to pickle "{}"'.format(output_path))
        vocab.save(output_path)
        vocab.save_probs(output_probs)



if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('root', help='Root of the Annotated English Gigaword (LDC12T21)', type=existsdir)
    p.add_argument('-v', action='count', help='Increase verbosity')
    p.add_argument('-o', '--output', required=True, help='Output path for the vocab.')
    p.add_argument('-op', '--output-probs', help='Calculate probabilities instead of raw counts')

    args = p.parse_args()

    if args.v == 1:
        LOG.setLevel(logging.INFO)
    elif args.v > 1:
        LOG.setLevel(logging.DEBUG)

    parse_corpus(args.root, args.output, args.output_probs)