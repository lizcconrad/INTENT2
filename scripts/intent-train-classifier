#!/usr/bin/env python3
"""
This script will take some number of Xigt-XML files as input,
extract word-level POS tags, and train a logistic regression
to classify them.
"""
import os, glob
import sys
from argparse import ArgumentParser
from typing import List

from intent2.model import LangWord, Instance
from intent2.processing import process_trans
from intent2.projection import clear_pos_tags, clear_bilingual_alignments, project_pos

from intent2.alignment import heuristic_alignment, AlignException
from sklearn.feature_extraction import DictVectorizer

from intent2.utils.cli_args import existsfile, existsdir, globfiles, get_dir_files
from intent2.serialize.importers import parse_xigt_corpus
from intent2.classification import describe_logreg, LRWrapper, PreTokenizedCountVectorizer, extract_gloss_word_feats
from xigt.codecs.xigtxml import load
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
import pickle

from intent2.utils.pos_tags import TagsetMapping, get_lg_tag

import logging
logging.basicConfig()
LOG = logging.getLogger()


def map_pos(pos, tagmap: TagsetMapping):
    returned = tagmap.get(pos, pos)
    return returned

def get_pattern(pattern):
    return glob.glob(pattern)

def get_projected_tags(inst: Instance, heur_list=None,
                       subword_multiple_alignment='precedence',
                       word_multiple_alignment='precedence' ) -> List[str]:
    # Get any (standard) projected tags, using multiple
    # alignment and other default settings
    clear_pos_tags(inst.lang)
    clear_pos_tags(inst.gloss)
    clear_pos_tags(inst.trans)
    process_trans(inst, parse=False)
    try:
        heuristic_alignment(inst, heur_list=heur_list)
        if inst.trans.alignments:
            project_pos(inst, subword_multiple_alignment=subword_multiple_alignment, word_multiple_alignment=word_multiple_alignment)
    except AlignException as ae:
        pass
    heur_tags = [gloss_w.pos for gloss_w in inst.gloss]
    return heur_tags

if __name__ == '__main__':
    p = ArgumentParser()
    p.add_argument('-f', '--file', action='append', help='Provide a particular file for input', type=existsfile, default=[])
    p.add_argument('-ef', '--exclude-file', action='append', help='Exclude certain files from training', type=existsfile, default=[])
    p.add_argument('-p', '--pattern', help='Add a glob pattern.', type=globfiles, default=[])
    p.add_argument('-d', '--dir', action='append', help='Specify a directory containing the files.', type=existsdir, default=[])
    p.add_argument('-r', '--recursive', action='store_true', help='If a directory is specified, recursively search')
    p.add_argument('-o', '--output', help='Store the classifier', required=True)
    p.add_argument('--vocab', help='Use a dictionary of word--tag probabilities to help with unigram POS probs.')
    p.add_argument('-vf', '--vectors', help='Log the training vectors to this file.')
    p.add_argument('-tf', help='Output text+labels for possible CNN training')
    p.add_argument('-v', '--verbose', help='Increase verbosity', action='count', default=0)
    p.add_argument('--tagmap', help='Map POS tags in the training data using this tagmap.', type=TagsetMapping.load, default={})
    p.add_argument('--maxiter', help='Max iterations for training.', default=1000)
    p.add_argument('--use-pt', help='Use projected tags as features in training the classifier.', action='store_true', default=False)
    p.add_argument('--use-pst', help='Use projected sub-tags as features in training the classifier.', action='store_true', default=False)
    p.add_argument('--no-vocab', help="Don't use the dictionary lookup for words features.", action='store_false', default=True)

    p.add_argument('--method', choices=['gold', 'proj', 'both'], help='Use gold-standard annotations, high-precision heuristic alignments, or both.', default='gold')

    args = p.parse_args()

    # The files to process can be any combination of globs, a directory, or
    pathlist = args.file + list(get_dir_files(args.dir, ext_filter='.xml', recursive=args.recursive)) + args.pattern
    for path in args.exclude_file:
        pathlist.remove(path)
    if not pathlist:
        print('No files were found. Please check your args and try again.')
        sys.exit(1)

    # Set verbosity
    if args.verbose == 1:
        LOG.setLevel(logging.INFO)
    if args.verbose >= 2:
        LOG.setLevel(logging.DEBUG)



    X_text = []
    y = []

    vocab = {}
    if args.vocab:
        with open(args.vocab, 'rb') as f:
            vocab = pickle.load(f)


    # Use gold tags
    use_gold_tags = args.method in {'gold', 'both'}
    use_proj_tags = args.method in {'proj', 'both'}

    # Load the files.
    for path in pathlist:
        with open(path, 'r') as xigt_f:
            xc = load(xigt_f)
            c = parse_xigt_corpus(xc)
            for inst in c:

                if not (inst.trans and inst.gloss):
                    continue

                # Get any existing gold tags and save them.
                gold_tags = [get_lg_tag(gloss_w) for gloss_w in inst.gloss]

                # Don't continue with getting projected tags and other analysis
                # if we are only using gold tags, and none are present.
                if not list(filter(lambda x: x, gold_tags)) and args.method == 'gold':
                    continue

                # Get default projected tags
                heur_tags = get_projected_tags(inst) if (args.use_pt or args.use_pst) else [None] * len(inst.gloss)

                # Get high-precision projected tags
                high_prec_heur_tags = get_projected_tags(inst, heur_list=['exact'], word_multiple_alignment='same', subword_multiple_alignment='same') if use_proj_tags else []


                # Collect features from the instances.
                if inst.gloss:

                    inst_X = []
                    gold_y = []
                    proj_y = []

                    # Go through and collect basic training features
                    for gloss_w, heur_tag in zip(inst.gloss, heur_tags):
                        gloss_w_feats = extract_gloss_word_feats(gloss_w, vocab,
                                                                 projected_tag=heur_tag if args.use_pt else None,
                                                                 subword_tags=[gsw.pos for gsw in gloss_w.subwords if gsw.pos] if args.use_pst else [],
                                                                 use_vocab=args.no_vocab)
                        inst_X.append(gloss_w_feats)

                        # Use existing gold tags from the instance if
                        # the label extraction method is either "gold" or "both"
                        if use_gold_tags:
                            lg_tag = get_lg_tag(gloss_w)  # Use lang POS tags over gloss if present.
                            gold_y.append(map_pos(lg_tag, args.tagmap))

                    # Use (high-precision) heuristically projected tags for training labels
                    # if the label extraction method is either "aln" or "both"
                    if use_proj_tags and inst.trans:
                        proj_y.extend([map_pos(tag, args.tagmap) for tag in high_prec_heur_tags])

                    # Iterate through a list of features/labels,
                    # and only add to the set of training instances
                    # if there are both features for the instance and
                    # a valid label.
                    def add_tags(X_iter, y_iter):
                        for X_elt, y_elt in zip(X_iter, y_iter):
                            if X_elt and y_elt:
                                X_text.append(X_elt)
                                y.append(y_elt)

                    # In the case of "both," zero out the projected tags
                    # when there is a supervised tag provided.
                    if gold_y and proj_y:
                        assert len(gold_y) == len(proj_y)
                        for i in range(len(gold_y)):
                            if gold_y[i]:
                                proj_y[i] = None

                    add_tags(inst_X, gold_y)
                    add_tags(inst_X, proj_y)



    # Process the extracted feats into vectors
    vectorizer = DictVectorizer()
    X_vecs = vectorizer.fit_transform(X_text)
    if args.vectors:
        with open(args.vectors, 'w') as vec_f:
            for label, x_feats in zip(y, X_text):
                vec_f.write('{}\t{}\n'.format(label, ' '.join(['{}:{:.3f}'.format(key, val)
                                                               for key, val in x_feats.items()])
                                              ))

    # Train the logistic regression classifier
    print('Training classifier using {} training instances'.format(len(X_text)))
    lr = LogisticRegressionCV(solver='saga', multi_class='ovr', cv=5, max_iter=10000)
    lr.fit(X_vecs, y)

    describe_logreg(lr, vectorizer)

    # Save the model
    lr = LRWrapper(lr, vectorizer, vocab)
    lr.save(args.output)
